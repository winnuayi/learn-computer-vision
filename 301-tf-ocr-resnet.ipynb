{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/agoila/lisa-faster-R-CNN/blob/master/pyimagesearch/nn/conv/resnet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 06:00:02.404480: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-07 06:00:02.529176: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-12-07 06:00:02.529203: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-12-07 06:00:03.664286: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-07 06:00:03.664353: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-12-07 06:00:03.664361: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# import the necessary packages\n",
    "# from keras.layers.normalization import batch_normalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import AveragePooling2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.convolutional import ZeroPadding2D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "from keras.layers import add\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "class ResNet:\n",
    "\t@staticmethod\n",
    "\tdef residual_module(data, K, stride, chanDim, red=False,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9):\n",
    "\t\t# the shortcut branch of the ResNet module should be\n",
    "\t\t# initialize as the input (identity) data\n",
    "\t\tshortcut = data\n",
    "\n",
    "\t\t# the first block of the ResNet module are the 1x1 CONVs\n",
    "\t\tbn1 = tf.keras.layers.BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(data)\n",
    "\t\tact1 = Activation(\"relu\")(bn1)\n",
    "\t\tconv1 = Conv2D(int(K * 0.25), (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# the second block of the ResNet module are the 3x3 CONVs\n",
    "\t\tbn2 = tf.keras.layers.BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv1)\n",
    "\t\tact2 = Activation(\"relu\")(bn2)\n",
    "\t\tconv2 = Conv2D(int(K * 0.25), (3, 3), strides=stride,\n",
    "\t\t\tpadding=\"same\", use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act2)\n",
    "\n",
    "\t\t# the third block of the ResNet module is another set of 1x1\n",
    "\t\t# CONVs\n",
    "\t\tbn3 = tf.keras.layers.BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(conv2)\n",
    "\t\tact3 = Activation(\"relu\")(bn3)\n",
    "\t\tconv3 = Conv2D(K, (1, 1), use_bias=False,\n",
    "\t\t\tkernel_regularizer=l2(reg))(act3)\n",
    "\n",
    "\t\t# if we are to reduce the spatial size, apply a CONV layer to\n",
    "\t\t# the shortcut\n",
    "\t\tif red:\n",
    "\t\t\tshortcut = Conv2D(K, (1, 1), strides=stride,\n",
    "\t\t\t\tuse_bias=False, kernel_regularizer=l2(reg))(act1)\n",
    "\n",
    "\t\t# add together the shortcut and the final CONV\n",
    "\t\tx = add([conv3, shortcut])\n",
    "\n",
    "\t\t# return the addition as the output of the ResNet module\n",
    "\t\treturn x\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build(width, height, depth, classes, stages, filters,\n",
    "\t\treg=0.0001, bnEps=2e-5, bnMom=0.9, dataset=\"cifar\"):\n",
    "\t\t# initialize the input shape to be \"channels last\" and the\n",
    "\t\t# channels dimension itself\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "\t\t# if we are using \"channels first\", update the input shape\n",
    "\t\t# and channels dimension\n",
    "\t\tif K.image_data_format() == \"channels_first\":\n",
    "\t\t\tinputShape = (depth, height, width)\n",
    "\t\t\tchanDim = 1\n",
    "\n",
    "\t\t# set the input and apply BN\n",
    "\t\tinputs = Input(shape=inputShape)\n",
    "\t\tx = tf.keras.layers.BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(inputs)\n",
    "\n",
    "\t\t# check if we are utilizing the CIFAR dataset\n",
    "\t\tif dataset == \"cifar\":\n",
    "\t\t\t# apply a single CONV layer\n",
    "\t\t\tx = Conv2D(filters[0], (3, 3), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\n",
    "\t\t# check to see if we are using the Tiny ImageNet dataset\n",
    "\t\telif dataset == \"tiny_imagenet\":\n",
    "\t\t\t# apply CONV => BN => ACT => POOL to reduce spatial size\n",
    "\t\t\tx = Conv2D(filters[0], (5, 5), use_bias=False,\n",
    "\t\t\t\tpadding=\"same\", kernel_regularizer=l2(reg))(x)\n",
    "\t\t\tx = tf.keras.layers.BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\t\tmomentum=bnMom)(x)\n",
    "\t\t\tx = Activation(\"relu\")(x)\n",
    "\t\t\tx = ZeroPadding2D((1, 1))(x)\n",
    "\t\t\tx = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "\n",
    "\t\t# loop over the number of stages\n",
    "\t\tfor i in range(0, len(stages)):\n",
    "\t\t\t# initialize the stride, then apply a residual module\n",
    "\t\t\t# used to reduce the spatial size of the input volume\n",
    "\t\t\tstride = (1, 1) if i == 0 else (2, 2)\n",
    "\t\t\tx = ResNet.residual_module(x, filters[i + 1], stride,\n",
    "\t\t\t\tchanDim, red=True, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t\t# loop over the number of layers in the stage\n",
    "\t\t\tfor j in range(0, stages[i] - 1):\n",
    "\t\t\t\t# apply a ResNet module\n",
    "\t\t\t\tx = ResNet.residual_module(x, filters[i + 1],\n",
    "\t\t\t\t\t(1, 1), chanDim, bnEps=bnEps, bnMom=bnMom)\n",
    "\n",
    "\t\t# apply BN => ACT => POOL\n",
    "\t\tx = tf.keras.layers.BatchNormalization(axis=chanDim, epsilon=bnEps,\n",
    "\t\t\tmomentum=bnMom)(x)\n",
    "\t\tx = Activation(\"relu\")(x)\n",
    "\t\tx = AveragePooling2D((8, 8))(x)\n",
    "\n",
    "\t\t# softmax classifier\n",
    "\t\tx = Flatten()(x)\n",
    "\t\tx = Dense(classes, kernel_regularizer=l2(reg))(x)\n",
    "\t\tx = Activation(\"softmax\")(x)\n",
    "\n",
    "\t\t# create the model\n",
    "\t\tmodel = Model(inputs, x, name=\"resnet\")\n",
    "\n",
    "\t\t# return the constructed network architecture\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading datasets...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 8s 1us/step\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from imutils import build_montages\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "import matplotlib; matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import tensorflow_datasets as tfds\n",
    "\n",
    "# from models.pyimagesearch import resnet\n",
    "# from keras.layers.normalization import batch_normalization\n",
    "\n",
    "def load_az_dataset(dataset_path):\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    # loop over the rows of the A-Z handwritten digit dataset\n",
    "    for row in open(dataset_path):\n",
    "        # parse the label and image from the row\n",
    "        row = row.split(\",\")\n",
    "        # first column for label\n",
    "        label = int(row[0])\n",
    "        # the rest (784 columns) for image\n",
    "        image = np.array([int(x) for x in row[1:]], dtype=\"uint8\")\n",
    "\n",
    "        # images are represented as single channel (grayscale) images\n",
    "        # that are 28x28=784 pixels -- we need to take this flattened\n",
    "        # 784-d list of numbers and reshape them into a 28x28 matrix\n",
    "        image = image.reshape((28, 28))\n",
    "\n",
    "        data.append(image)\n",
    "        labels.append(label)\n",
    "    \n",
    "    data = np.array(data, dtype='float32')\n",
    "    labels = np.array(labels, dtype='int')\n",
    "\n",
    "    return (data, labels)\n",
    "\n",
    "\n",
    "def load_mnist_dataset():\n",
    "    ((train_data, train_labels), (test_data, test_labels)) = mnist.load_data()\n",
    "    data = np.vstack([train_data, test_data])\n",
    "    labels = np.hstack([train_labels, test_labels])\n",
    "\n",
    "    return (data, labels)\n",
    "\n",
    "PATH = 'data/A_Z Handwritten Data.csv'\n",
    "\n",
    "print(\"[INFO] loading datasets...\")\n",
    "(az_data, az_labels) = load_az_dataset(PATH)\n",
    "(digits_data, digits_labels) = load_mnist_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442451, 28, 28) (442451,)\n"
     ]
    }
   ],
   "source": [
    "az_labels += 10\n",
    "\n",
    "data = np.vstack([az_data, digits_data])\n",
    "labels = np.hstack([az_labels, digits_labels])\n",
    "\n",
    "print(data.shape, labels.shape)\n",
    "\n",
    "data = [cv2.resize(image, (32, 32)) for image in data]\n",
    "data = np.array(data, dtype='float32')\n",
    "\n",
    "data = np.expand_dims(data, axis=-1)\n",
    "data /= 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ciheul/Projects/ml/env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "le = LabelBinarizer()\n",
    "labels = le.fit_transform(labels)\n",
    "counts = labels.sum(axis=0)\n",
    "\n",
    "class_totals = labels.sum(axis=0)\n",
    "class_weight = {}\n",
    "\n",
    "for i in range(0, len(class_totals)):\n",
    "    class_weight[i] = class_totals.max() / class_totals[i]\n",
    "\n",
    "(train_x, test_x, train_y, test_y) = train_test_split(\n",
    "    data, labels, test_size=0.20, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.05,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "print(le.classes_)\n",
    "EPOCHS = 10\n",
    "INIT_LR = 1e-1\n",
    "BS = 128\n",
    "\n",
    "opt = SGD(lr=INIT_LR, decay=INIT_LR/EPOCHS)\n",
    "model = ResNet.build(32, 32, 1, len(le.classes_), (3, 3, 3), (64, 64, 128, 256),\n",
    "                     reg=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/3\n",
      "2765/2765 [==============================] - ETA: 0s - loss: 3.5768 - accuracy: 0.8090"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-07 09:35:39.156674: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 362459136 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2765/2765 [==============================] - 3210s 1s/step - loss: 3.5768 - accuracy: 0.8090 - val_loss: 0.6461 - val_accuracy: 0.8865\n",
      "Epoch 2/3\n",
      "2765/2765 [==============================] - 3208s 1s/step - loss: 1.8737 - accuracy: 0.8797 - val_loss: 0.5991 - val_accuracy: 0.8911\n",
      "Epoch 3/3\n",
      "2765/2765 [==============================] - 3233s 1s/step - loss: 1.6888 - accuracy: 0.8885 - val_loss: 0.5823 - val_accuracy: 0.8884\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] training network...\")\n",
    "H = model.fit(\n",
    "    aug.flow(train_x, train_y, batch_size=BS),\n",
    "    validation_data=(test_x, test_y),\n",
    "    steps_per_epoch=len(train_x) // BS,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weight,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "692/692 [==============================] - 146s 210ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.18      0.77      0.30      1381\n",
      "           1       0.97      0.96      0.97      1575\n",
      "           2       0.85      0.90      0.88      1398\n",
      "           3       0.93      0.98      0.95      1428\n",
      "           4       0.80      0.91      0.85      1365\n",
      "           5       0.49      0.82      0.61      1263\n",
      "           6       0.89      0.95      0.92      1375\n",
      "           7       0.90      0.96      0.93      1459\n",
      "           8       0.84      0.95      0.89      1365\n",
      "           9       0.91      0.97      0.94      1392\n",
      "           A       0.97      0.97      0.97      2774\n",
      "           B       0.96      0.94      0.95      1734\n",
      "           C       0.98      0.96      0.97      4682\n",
      "           D       0.89      0.91      0.90      2027\n",
      "           E       0.99      0.95      0.97      2288\n",
      "           F       0.91      0.99      0.95       232\n",
      "           G       0.86      0.92      0.89      1152\n",
      "           H       0.93      0.95      0.94      1444\n",
      "           I       0.92      0.97      0.95       224\n",
      "           J       0.91      0.93      0.92      1699\n",
      "           K       0.94      0.94      0.94      1121\n",
      "           L       0.97      0.97      0.97      2317\n",
      "           M       0.97      0.96      0.96      2467\n",
      "           N       0.98      0.97      0.98      3802\n",
      "           O       0.95      0.58      0.72     11565\n",
      "           P       1.00      0.96      0.98      3868\n",
      "           Q       0.86      0.94      0.90      1162\n",
      "           R       0.98      0.96      0.97      2313\n",
      "           S       0.97      0.86      0.91      9684\n",
      "           T       0.99      0.96      0.98      4499\n",
      "           U       0.97      0.95      0.96      5802\n",
      "           V       0.94      0.98      0.96       836\n",
      "           W       0.95      0.96      0.96      2157\n",
      "           X       0.97      0.98      0.97      1254\n",
      "           Y       0.96      0.88      0.92      2172\n",
      "           Z       0.91      0.90      0.90      1215\n",
      "\n",
      "    accuracy                           0.89     88491\n",
      "   macro avg       0.90      0.93      0.91     88491\n",
      "weighted avg       0.93      0.89      0.90     88491\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label_names = \"0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "label_names = [l for l in label_names]\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(test_x, batch_size=BS)\n",
    "print(classification_report(test_y.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1),\n",
    "                            target_names=label_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] serializing network...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe3163b3ee0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model to disk\n",
    "print(\"[INFO] serializing network...\")\n",
    "model.save(\"handwriting\", save_format=\"h5\")\n",
    "\n",
    "# construct a plot that plots and saves the training history\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss and Accuracy\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "# plt.savefig(args[\"plot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88491, 32, 32, 1)\n",
      "(88491, 36)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 119ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.6.0) /io/opencv/modules/imgproc/src/resize.cpp:3689: error: (-215:Assertion failed) !dsize.empty() in function 'resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m     color \u001b[39m=\u001b[39m (\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m255\u001b[39m)\n\u001b[1;32m     14\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mmerge([image] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mresize(image, (\u001b[39m96\u001b[39;49m, \u001b[39m96\u001b[39;49m), interpolation\u001b[39m=\u001b[39;49mcv2\u001b[39m.\u001b[39;49mINTER_LINEAR)\n\u001b[1;32m     16\u001b[0m cv2\u001b[39m.\u001b[39mputText(image, label, (\u001b[39m5\u001b[39m, \u001b[39m20\u001b[39m), cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[39m0.75\u001b[39m, color, \u001b[39m2\u001b[39m)\n\u001b[1;32m     18\u001b[0m images\u001b[39m.\u001b[39mappend(image)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.6.0) /io/opencv/modules/imgproc/src/resize.cpp:3689: error: (-215:Assertion failed) !dsize.empty() in function 'resize'\n"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "\n",
    "for i in np.random.choice(np.arange(0, len(test_y)), size=(49,)):\n",
    "    probs = model.predict(test_x[np.newaxis, i])\n",
    "    prediction = probs.argmax(axis=1)\n",
    "    label = label_names[prediction[0]]\n",
    "\n",
    "    image = (test_x * 255).astype(\"uint8\")\n",
    "    color = (0, 255, 0)\n",
    "\n",
    "    if prediction[0] != np.argmax(test_y[i]):\n",
    "        color = (0, 0, 255)\n",
    "    \n",
    "    image = cv2.merge([image] * 3)\n",
    "    image = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
    "    cv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
    "\n",
    "    images.append(image)\n",
    "\n",
    "montage = build_montages(images, (96, 96), (7, 7))[0]\n",
    "cv2.imshow(\"OCR Results\", montage)\n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bb59aeb8aa62b48260650b7f57d5e74e5214161a5d0da408bb8793b8689a970e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
